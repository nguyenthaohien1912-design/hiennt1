# python.py

import streamlit as st
from docx import Document
import unicodedata, re, os
from difflib import get_close_matches, SequenceMatcher

# =======================
# C·∫•u h√¨nh trang
# =======================
st.set_page_config(
    page_title="S·ªï tay h∆∞·ªõng d·∫´n ki·ªÉm tra Agribank H√† Th√†nh",
    page_icon="üìò",
    layout="wide"
)

# =======================
# CSS ch·ªß ƒë·ªÅ Agribank
# =======================
st.markdown("""
<style>
.main { background:#fff; color:#222; font-family:'Segoe UI', sans-serif; }
.block-container { padding-top:1rem; padding-bottom:1rem; }
h1,h2,h3,h4 { color:#800000 !important; }
.sidebar .sidebar-content { background:#8B0000; color:#fff; }
.stButton button { background:#800000; color:#fff; border-radius:6px; border:none; padding:.5rem 1rem; }
.stButton button:hover { background:#a00000; color:#fff; }
</style>
""", unsafe_allow_html=True)

# =======================
# Logo + ti√™u ƒë·ªÅ
# =======================
col1, col2 = st.columns([0.15, 0.85])
with col1:
    # d√πng logo c·ª•c b·ªô n·∫øu c√≥, fallback sang online ƒë·ªÉ tr√°nh l·ªói
    logo_path = "logo_agribank.png"
    if os.path.exists(logo_path):
        st.image(logo_path, use_column_width=True)
    else:
        st.image("https://upload.wikimedia.org/wikipedia/commons/4/4b/Agribank_logo.png",
                 use_column_width=True)
with col2:
    st.title("üìò S·ªî TAY H∆Ø·ªöNG D·∫™N KI·ªÇM TRA NGHI·ªÜP V·ª§")
    st.subheader("Agribank Chi nh√°nh H√† Th√†nh ‚Äì Phi√™n b·∫£n s·ªë h√≥a")
st.markdown("---")

# =======================
# Ti·ªán √≠ch vƒÉn b·∫£n
# =======================
def normalize_text(text: str) -> str:
    """B·ªè d·∫•u ti·∫øng Vi·ªát + ƒë∆∞a v·ªÅ lower ƒë·ªÉ so kh·ªõp kh√¥ng d·∫•u."""
    nfkd = unicodedata.normalize('NFKD', text)
    return ''.join(c for c in nfkd if not unicodedata.combining(c)).lower()

@st.cache_data
def load_docx(file_path):
    from docx import Document
    import os
    if not os.path.exists(file_path):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {file_path}")
        st.stop()

    doc = Document(file_path)
    chapters = {}
    current_chapter = "Kh√°c"

    def extract_text_from_table(table):
        """ƒê·ªçc to√†n b·ªô n·ªôi dung t·ª´ b·∫£ng v√† n·ªëi l·∫°i th√†nh c√°c d√≤ng vƒÉn b·∫£n"""
        rows = []
        for row in table.rows:
            # L·∫•y text t·ª´ t·ª´ng cell trong b·∫£ng
            cells = [cell.text.strip() for cell in row.cells if cell.text.strip()]
            if cells:
                rows.append(" | ".join(cells))
        return rows

    # ƒê·ªçc to√†n b·ªô ph·∫ßn th√¢n t√†i li·ªáu (paragraphs + tables)
    for block in doc.element.body:
        # ƒêo·∫°n vƒÉn
        if block.tag.endswith('p'):
            for p in doc.paragraphs:
                text = p.text.strip()
                if not text:
                    continue
                if text.lower().startswith("ch∆∞∆°ng"):
                    current_chapter = text
                    chapters[current_chapter] = []
                else:
                    chapters.setdefault(current_chapter, []).append(text)
            break  # tr√°nh ƒë·ªçc l·∫°i c√°c ƒëo·∫°n tr√πng
        # B·∫£ng
        elif block.tag.endswith('tbl'):
            # t√¨m t·∫•t c·∫£ b·∫£ng
            for table in doc.tables:
                for t in extract_text_from_table(table):
                    chapters.setdefault(current_chapter, []).append(t)
            break

    return chapters
    
# =======================
# N·∫°p d·ªØ li·ªáu
# =======================
FILENAME = "So_tay_Agribank.docx.docx"   # ƒë·ªïi t√™n file t·∫°i ƒë√¢y n·∫øu c·∫ßn
chapters = load_docx(FILENAME)

# Chu·∫©n b·ªã corpus ph·∫≥ng ƒë·ªÉ fuzzy/g·ª£i √Ω
@st.cache_data
def build_corpus(chapters_dict):
    rows = []
    for ch, paras in chapters_dict.items():
        for p in paras:
            rows.append({
                "chapter": ch,
                "text": p,
                "norm": normalize_text(p)
            })
    return rows

corpus = build_corpus(chapters)

# T·ª´ ƒëi·ªÉn ƒë·ªìng nghƒ©a/bi·∫øn th·ªÉ hay d√πng (c√≥ th·ªÉ b·ªï sung d·∫ßn)
SYNONYMS = {
    "ho so cap tin dung": [
        "h·ªì s∆° c·∫•p t√≠n d·ª•ng", "h·ªì s∆° vay v·ªën", "b·ªô h·ªì s∆° t√≠n d·ª•ng",
        "h·ªì s∆° cho vay", "h·ªì s∆° kho·∫£n vay", "h·ªì s∆° t√≠n d·ª•ng"
    ],
    "tin dung": ["t√≠n d·ª•ng", "cho vay", "kho·∫£n vay", "c·∫•p t√≠n d·ª•ng"],
    "bao dam": ["b·∫£o ƒë·∫£m", "t√†i s·∫£n b·∫£o ƒë·∫£m", "tsbƒë", "th·∫ø ch·∫•p", "c·∫ßm c·ªë"],
    "thanh toan": ["thanh to√°n", "k·∫ø to√°n", "ch·ª©ng t·ª´", "h·∫°ch to√°n"],
}

def expand_query(q: str):
    base = normalize_text(q)
    variants = {q}  # gi·ªØ nguy√™n b·∫£n c√≥ d·∫•u
    variants.add(base)  # b·∫£n kh√¥ng d·∫•u
    for key, alts in SYNONYMS.items():
        if key in base:
            variants.update(alts)
            variants.update([normalize_text(a) for a in alts])
    return list({v for v in variants if v})

def highlight(text: str, variants):
    """T√¥ ƒë·∫≠m t·∫•t c·∫£ bi·∫øn th·ªÉ (c√≥ d·∫•u & kh√¥ng d·∫•u)."""
    # pattern gh√©p OR cho c√°c bi·∫øn th·ªÉ c√≥ d·∫•u
    with_diacritics = [v for v in variants if any("ƒÉ√¢ƒë√™√¥∆°∆∞√°√†·∫£√£·∫°√©√®·∫ª·∫Ω·∫π√≥√≤·ªè√µ·ªç√∫√π·ªß≈©·ª•√≠√¨·ªâƒ©·ªã√Ω·ª≥·ª∑·ªπ·ªµ" in v.lower() for _ in [0])]
    # t√¥ ƒë·∫≠m b·∫£n c√≥ d·∫•u tr∆∞·ªõc:
    if with_diacritics:
        pattern = r"(" + "|".join(re.escape(v) for v in sorted(with_diacritics, key=len, reverse=True)) + r")"
        text = re.sub(pattern, r"**\1**", text, flags=re.IGNORECASE)
    # n·∫øu kh√¥ng c√≥ d·∫•u trong bi·∫øn th·ªÉ, b·ªè qua highlight kh√¥ng d·∫•u ƒë·ªÉ tr√°nh b√¥i ƒë·∫≠m sai v·ªã tr√≠
    return text

def search_documents(query: str, scope_dict, use_synonyms=True):
    """T√¨m trong scope_dict: {chapter:[paras]} -> tr·∫£ v·ªÅ {chapter:[paras match]}"""
    variants = expand_query(query) if use_synonyms else [query, normalize_text(query)]
    norm_variants = [normalize_text(v) for v in variants]
    results = {}
    for ch, paras in scope_dict.items():
        hits = []
        for p in paras:
            pn = normalize_text(p)
            if any(v in pn for v in norm_variants):
                hits.append(p)
        if hits:
            results[ch] = hits
    return results, variants

# =======================
# Sidebar
# =======================
sb_logo = "logo_agribank.png"
if os.path.exists(sb_logo):
    st.sidebar.image(sb_logo, use_column_width=True)
else:
    st.sidebar.image("https://upload.wikimedia.org/wikipedia/commons/4/4b/Agribank_logo.png",
                     use_column_width=True)

st.sidebar.markdown("### üìë **Danh m·ª•c ch∆∞∆°ng**")
chapter_list = list(chapters.keys())
selected_chapter = st.sidebar.selectbox("Ch·ªçn ch∆∞∆°ng:", chapter_list)

st.sidebar.markdown("---")
st.sidebar.markdown("### üí¨ **Chatbot h∆∞·ªõng d·∫´n ki·ªÉm tra**")
query = st.sidebar.text_input("Nh·∫≠p t·ª´ kh√≥a/c√¢u h·ªèi (VD: h·ªì s∆° c·∫•p t√≠n d·ª•ng, ch·ª©ng t·ª´...)")
search_in_current = st.sidebar.checkbox("üîç Ch·ªâ t√¨m trong ch∆∞∆°ng ƒë√£ ch·ªçn", value=False)
use_syn = st.sidebar.checkbox("‚ú® M·ªü r·ªông t·ª´ ƒë·ªìng nghƒ©a", value=True)

st.sidebar.markdown("---")
with open(FILENAME, "rb") as f:
    st.sidebar.download_button(
        label="‚¨áÔ∏è T·∫£i S·ªï tay g·ªëc (.docx)",
        data=f,
        file_name="So_tay_Agribank.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    )

# =======================
# N·ªôi dung ch∆∞∆°ng ƒë√£ ch·ªçn
# =======================
st.header(f"üìÇ {selected_chapter}")
for para in chapters[selected_chapter]:
    st.markdown(f"- {para}")

# =======================
# T√¨m ki·∫øm n√¢ng cao + g·ª£i √Ω
# =======================
if query:
    st.markdown("---")
    st.subheader(f"üîé K·∫øt qu·∫£ t√¨m ki·∫øm cho: *{query}*")

    scope = {selected_chapter: chapters[selected_chapter]} if search_in_current else chapters
    results_by_chapter, variants = search_documents(query, scope, use_synonyms=use_syn)

    if results_by_chapter:
        for ch, paras in results_by_chapter.items():
            with st.expander(f"üìÅ {ch} ({len(paras)} k·∫øt qu·∫£)", expanded=True):
                for p in paras:
                    st.markdown(f"üîπ {highlight(p, variants)}")
    else:
        st.info("Kh√¥ng t√¨m th·∫•y n·ªôi dung kh·ªõp ho√†n to√†n. D∆∞·ªõi ƒë√¢y l√† c√°c **g·ª£i √Ω g·∫ßn ƒë√∫ng**:")
        # G·ª£i √Ω g·∫ßn ƒë√∫ng t·ª´ to√†n b·ªô corpus ƒë√£ chu·∫©n h√≥a
        full_texts = [row["text"] for row in corpus]
        # L·∫•y 10 c√¢u/ƒëo·∫°n gi·ªëng nh·∫•t theo ratio
        scored = sorted(
            [(t, SequenceMatcher(None, normalize_text(query), normalize_text(t)).ratio()) for t in full_texts],
            key=lambda x: x[1],
            reverse=True
        )[:10]
        for t, score in scored:
            # ch·ªâ hi·ªÉn th·ªã g·ª£i √Ω ƒë·ªß ‚Äúgi·ªëng‚Äù
            if score >= 0.45:
                st.markdown(f"üí° {t}  \n&emsp;`similarity: {score:.2f}`")
        if not scored or scored[0][1] < 0.45:
            st.write("‚Ä¢ Th·ª≠ r√∫t g·ªçn t·ª´ kh√≥a (vd: *h·ªì s∆°*, *t√≠n d·ª•ng*, *cho vay*, *ch·ª©ng t·ª´*).")
